# GenAI-and-LLMS
This is a repository for resources and projects on GenAI and LLMs

# An overview of GenAI 
[A Primer on Generative AI (GenAI)](https://hashcollision.substack.com/p/a-primer-on-generative-ai-genai?r=854n6&utm_campaign=post&utm_medium=web&triedRedirect=true)
![image](https://github.com/ElahehJafarigol/GenAI-and-LLMS/assets/64182149/5dc8e972-3019-423c-b3bb-9adc2dfe5d20)

# Reading list
I Found this post on LinkedIn: "Ilya Sutskever of OpenAI gave John Carmack the following reading list of approximately 30 research papers and said, â€˜If you really learn all of these, youâ€™ll know 90% of what matters today in AI.â€™ I have added a few more LLM papers that potentially fill the remaining ~9%" by Bhairav M.

1. [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
2. [The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)
3. [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
4. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
5. [Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329)
6. [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
7. [Pointer Networks](https://arxiv.org/pdf/1506.03134)
8. [ImageNet Classification with Deep CNNs](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
9. [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/pdf/1511.06391)
10. [GPipe: Efficient Training of Giant Neural Networks](https://arxiv.org/pdf/1811.06965)
11. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)
12. [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/pdf/1511.07122)
13. [Neural Quantum Chemistry](https://arxiv.org/pdf/1704.01212)
14. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
15. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)
16. [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027)
17. [A Simple NN Module for Relational Reasoning](https://arxiv.org/pdf/1706.01427)
18. [Variational Lossy Autoencoder](https://arxiv.org/pdf/1611.02731)
19. [Relational RNNs](https://arxiv.org/pdf/1806.01822)
20. [Quantifying the Rise and Fall of Complexity in Closed Systems](https://arxiv.org/pdf/1405.6903)
21. [Neural Turing Machines](https://arxiv.org/pdf/1410.5401)
22. [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/pdf/1512.02595)
23. [Scaling Laws for Neural LMs](https://arxiv.org/pdf/2001.08361)
24. [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/pdf/math/0406077)
25. [Machine Super Intelligence Dissertation](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf)
26. [PAGE 434 onwards: Komogrov Complexity](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)
27. [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)
28. [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
29. [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)
30. [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2404.19756)

# The GPT, Llama, and Gemini papers:
1. [GPT-1](https://lnkd.in/gJ5Pe3HG)
2. [GPT-2](https://lnkd.in/gatQi8Ud)
3. [GPT-3](https://lnkd.in/g43GzYfZ)
4. [GPT-4](https://lnkd.in/ga_xEpEj)
5. [Llama-2](https://lnkd.in/gutaGW8h)
6. [Tools](https://lnkd.in/gqJ3aXpS)
7. [Gemini-Pro-1.5](https://lnkd.in/gbDcYp89)

[DeepLearning.AI: Agentic Design Patterns Part 1](https://lnkd.in/gphZ6Y5s)
Four AI agent strategies that improve GPT-4 and GPT-3.5 performance

# Surveys
1. [Large Language Models: A Survey](https://ar5iv.labs.arxiv.org/html/2402.06196)
2. [A Survey of Large Language Models](https://ar5iv.labs.arxiv.org/html/2303.18223)
3. [A Comprehensive Overview of Large Language Models](https://ar5iv.labs.arxiv.org/html/2307.06435)
