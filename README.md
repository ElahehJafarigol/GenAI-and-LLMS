# GenAI-and-LLMS
This is a repository for resources and projects on GenAI and LLMs

Found this post on LinkedIn: "Ilya Sutskever of OpenAI gave John Carmack the following reading list of approximately 30 research papers and said, â€˜If you really learn all of these, youâ€™ll know 90% of what matters today in AI.â€™ I have added a few more LLM papers that potentially fill the remaining ~9%" by Bhairav M.

The papers can be found here:
1. The Annotated Transformer https://nlp.seas.harvard.edu/annotated-transformer/
2. The First Law of Complexodynamics
3. The Unreasonable Effectiveness of RNNs
4. Understanding LSTM Networks
5. Recurrent Neural Network Regularization
6. Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
7. Pointer Networks
8. ImageNet Classification with Deep CNNs
9. Order Matters: Sequence to Sequence for Sets
10. GPipe: Efficient Training of Giant Neural Networks
11. Deep Residual Learning for Image Recognition
12. Multi-Scale Context Aggregation by Dilated Convolutions
13. Neural Quantum Chemistry
14. Attention Is All You Need
15. Neural Machine Translation by Jointly Learning to Align and Translate
16. Identity Mappings in Deep Residual Networks
17. A Simple NN Module for Relational Reasoning
18. Variational Lossy Autoencoder
19. Relational RNNs
20. Quantifying the Rise and Fall of Complexity in Closed Systems
21. Neural Turing Machines
22. Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
23. Scaling Laws for Neural LMs (arxiv.org)
24. A Tutorial Introduction to the Minimum Description Length Principle (arxiv.org)
25. Machine Super Intelligence Dissertation (vetta.org)
26. PAGE 434 onwards: Komogrov Complexity (lirmm.fr)
27. CS231n Convolutional Neural Networks for Visual Recognition (cs231n.github.io)
28. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ
29. BitNet: Scaling 1-bit Transformers for Large Language Models
30. KAN: Kolmogorov-Arnold Networks
